{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T00:39:20.054895100Z",
     "start_time": "2024-02-07T00:39:18.958157900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load modules and set configurations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, copy, random, pickle, gc\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdc1a6",
   "metadata": {},
   "source": [
    "# 5. LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce23af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=self.hidden_dim,\n",
    "      hidden_size=embedding_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "    x, (hidden_n, _) = self.rnn2(x)\n",
    "    return hidden_n.reshape((-1,1, self.embedding_dim))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, seq_len, input_dim=64, n_features=114):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=input_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.repeat(1,self.seq_len, 1)\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "    return self.output_layer(x)\n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "    super(RecurrentAutoencoder, self).__init__()\n",
    "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "with open(f'data-dict-for_lstm_ae.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d06ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = 1 # 1, 2, 3\n",
    "low_esi = 1 # 0, 1, 'all'\n",
    "data = data_dict[ver][low_esi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49fa7f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20],\n",
       "       [21],\n",
       "       [22],\n",
       "       [23]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['trn']['n_seq'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d161e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# select data\n",
    "ver = 1 # 1, 2, 3\n",
    "low_esi = 1 # 0, 1, 'all'\n",
    "data = data_dict[ver][low_esi]\n",
    "_, max_seq_len, n_var = data['trn']['X'].shape\n",
    "\n",
    "# make it as data loaders\n",
    "# building data loader\n",
    "data_loaders = {i:{} for i in ['trn', 'val_tr', 'val_th', 'tst']}\n",
    "for i in tqdm(['trn', 'val_tr', 'val_th', 'tst']):\n",
    "    tmp_X = torch.tensor(data[i]['X'].astype(np.float32))\n",
    "    tmp_y = torch.tensor(data[i]['y'].astype(int))\n",
    "    tmp_ids = torch.tensor(data[i]['ids'].astype(int))\n",
    "    tmp_n_seq = torch.tensor(data[i]['n_seq'].astype(int))\n",
    "    \n",
    "    batch_size = 256 # 256, 128, 64\n",
    "    data_loaders[i] = DataLoader(dataset=TensorDataset(tmp_X, tmp_y, tmp_ids, tmp_n_seq), batch_size=batch_size if i=='trn' else tmp_X.shape[0] if i=='var_tr' else 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29929eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {i:{} for i in ['trn', 'val_tr', 'val_th', 'tst']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a6f826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trn', 'val_tr', 'val_th', 'tst'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a30d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x25c9501d1c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders['val_tr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8681a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "seed_everything(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_unit = 256 # 64, 128, 256, 512\n",
    "n_epochs, factor, patience, min_lr = (5000, 0.1, 100, 1e-6)\n",
    "loss_reduction = 'global_mean'\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, min_lr=min_lr, verbose=True)\n",
    "criterion = nn.MSELoss(reduction='none').to(device)\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8478fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 20.742730674973455 val loss 12.271102830585246\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model = model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for seq_true, _, _, _ in data_loaders['trn']:\n",
    "        seq_true = seq_true.to(device)\n",
    "        mask = ~torch.all(seq_true==0, axis=2)\n",
    "        seq_pred = model(seq_true)\n",
    "\n",
    "        if loss_reduction == 'stay_wise_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "            lens = mask.sum(axis=1).detach().cpu()\n",
    "            c_lens = lens.cumsum(dim=0)\n",
    "            loss = 0\n",
    "            for idx, i in enumerate(c_lens):\n",
    "                s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                loss += l[s:i].sum()/lens[idx]\n",
    "        elif loss_reduction == 'global_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "            loss = l/mask.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    val_losses = []\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq_true, _, _, _ in data_loaders['val_tr']:\n",
    "            seq_true = seq_true.to(device)\n",
    "            mask = ~torch.all(seq_true==0, axis=2)\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            if loss_reduction == 'stay_wise_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "                lens = mask.sum(axis=1).detach().cpu()\n",
    "                c_lens = lens.cumsum(dim=0)\n",
    "                loss = 0\n",
    "                for idx, i in enumerate(lens):\n",
    "                    s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                    loss += l[idx].sum()/lens[idx]\n",
    "            elif loss_reduction == 'global_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "                loss = l/mask.sum()\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Current learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "    \n",
    "    print(f'early_stopping_counter: {early_stopping_counter} ')\n",
    "    \n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth')\n",
    "\n",
    "with open(f'model_history-lstm_ae-low_esi{ver}-{low_esi}.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d7b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40092/40092 [01:33<00:00, 426.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 256\n",
    "\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'val_th' #tst val_th\n",
    "\n",
    "eval_data = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, y, id, n_seq in tqdm(data_loaders[eval_split]):\n",
    "        id = id.cpu().numpy().ravel()[0]\n",
    "        y = y.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data.append([id, y, loss.item(), n_seq])\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data, columns=['id', 'true', 'score', 'n_seq'])\n",
    "eval_data.to_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6f47fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39344/39344 [01:32<00:00, 424.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 256\n",
    "\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'tst' #tst val_th\n",
    "\n",
    "eval_data = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, y, id, n_seq in tqdm(data_loaders[eval_split]):\n",
    "        id = id.cpu().numpy().ravel()[0]\n",
    "        y = y.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data.append([id, y, loss.item(), n_seq])\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data, columns=['id', 'true', 'score', 'n_seq'])\n",
    "eval_data.to_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a81de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40034/40034 [1:02:16<00:00, 10.71it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_split = 'val_th'\n",
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "eval_data = pd.read_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\")\n",
    "scores = eval_data['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data['pred'] = np.where(eval_data['score']>=s, 1, 0)\n",
    "    tmp = eval_data.groupby('id').agg({'true': lambda x: x.values[0], 'pred': 'max'}).reset_index()\n",
    "    tp, fp, fn, tn = conf_mat(tmp['true'], tmp['pred'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n",
    "eval_result.to_csv(f'eval_result-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e94e25",
   "metadata": {},
   "source": [
    "## ver 3, low_esi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68a1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 36.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# select data\n",
    "ver = 3 # 1, 2, 3\n",
    "low_esi = 1 # 0, 1, 'all'\n",
    "data = data_dict[ver][low_esi]\n",
    "_, max_seq_len, n_var = data['trn']['X'].shape\n",
    "\n",
    "# make it as data loaders\n",
    "# building data loader\n",
    "data_loaders = {i:{} for i in ['trn', 'val_tr', 'val_th', 'tst']}\n",
    "for i in tqdm(['trn', 'val_tr', 'val_th', 'tst']):\n",
    "    tmp_X = torch.tensor(data[i]['X'].astype(np.float32))\n",
    "    tmp_y = torch.tensor(data[i]['y'].astype(int))\n",
    "    tmp_ids = torch.tensor(data[i]['ids'].astype(int))\n",
    "    tmp_n_seq = torch.tensor(data[i]['n_seq'].astype(int))\n",
    "    \n",
    "    batch_size = 256 # 256, 128, 64\n",
    "    data_loaders[i] = DataLoader(dataset=TensorDataset(tmp_X, tmp_y, tmp_ids, tmp_n_seq), batch_size=batch_size if i=='trn' else tmp_X.shape[0] if i=='var_tr' else 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88b6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "seed_everything(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_unit = 256 # 64, 128, 256, 512\n",
    "n_epochs, factor, patience, min_lr = (5000, 0.1, 100, 1e-6)\n",
    "loss_reduction = 'global_mean'\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, min_lr=min_lr, verbose=True)\n",
    "criterion = nn.MSELoss(reduction='none').to(device)\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a74fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 29.29897666558987 val loss 25.937636117243397\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 2: train loss 28.305903737137957 val loss 25.699893199306285\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 3: train loss 28.2498651597558 val loss 25.696878304444446\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 4: train loss 26.824043390227526 val loss 21.962290358563983\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 5: train loss 23.34777094678181 val loss 19.326372412305215\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 6: train loss 20.41100878831817 val loss 16.79308662708971\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 7: train loss 18.16588385512189 val loss 15.117013239798768\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 8: train loss 15.668107800367402 val loss 12.228517960956472\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 9: train loss 14.640746058487311 val loss 10.51563122945746\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 10: train loss 11.633134743062461 val loss 8.236202640931841\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 11: train loss 10.715004665095632 val loss 7.902249456635823\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 12: train loss 10.09755576529154 val loss 7.455657139626398\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 13: train loss 9.338072922171616 val loss 6.555658886727068\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 14: train loss 8.937976424287005 val loss 6.099131320012872\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 15: train loss 8.121985074950427 val loss 5.223519489040441\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 16: train loss 7.605450705784123 val loss 5.099005658597732\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 17: train loss 7.063385835508021 val loss 4.496357204279957\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 18: train loss 7.068523735534854 val loss 4.261197205188979\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 19: train loss 6.176107200180612 val loss 3.9968868086117744\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 20: train loss 5.833779617053707 val loss 3.7469313655801386\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 21: train loss 5.580954060321901 val loss 3.566943466161508\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 22: train loss 5.550523731766678 val loss 3.6864672719222686\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 23: train loss 5.547695564060676 val loss 3.5011579763909904\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 24: train loss 5.0949219726934665 val loss 3.16390327262549\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 25: train loss 4.80584139649461 val loss 3.0232608429894547\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 26: train loss 4.8538119531259305 val loss 3.0167800142511805\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 27: train loss 5.960150814637905 val loss 3.2264280300683925\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 28: train loss 4.78444958314663 val loss 2.99305080781998\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 29: train loss 4.560328373094884 val loss 2.8035725765895347\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 30: train loss 4.125187193475118 val loss 2.6504977449697535\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 31: train loss 4.003490977170991 val loss 2.5975616455129598\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 32: train loss 4.024216300103722 val loss 2.5150721339936752\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 33: train loss 4.188197202798797 val loss 2.4855898968450765\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 34: train loss 4.207069469661247 val loss 2.4671644018400003\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 35: train loss 3.890406780126618 val loss 2.3717635044979257\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 36: train loss 3.479474522718569 val loss 2.3005831259590717\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 37: train loss 3.3705484474577556 val loss 2.224786466582709\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 38: train loss 3.2296765751954988 val loss 2.5842817566214396\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 39: train loss 3.4819392765440593 val loss 2.268482678555377\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 40: train loss 2.985056353778374 val loss 2.026603265882619\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 41: train loss 2.782398851906381 val loss 1.8710256156411877\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 42: train loss 2.514322291060192 val loss 1.7641126493978054\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 43: train loss 2.5059267311561397 val loss 1.7118089185393897\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 44: train loss 2.4468326772131572 val loss 1.7194861592025221\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 45: train loss 2.3829065270540193 val loss 1.6586270080356567\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 46: train loss 2.224974639532043 val loss 1.6244993747134784\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 47: train loss 2.0789639222912673 val loss 1.5791984658096379\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 48: train loss 2.2189146963561455 val loss 1.5706855436748406\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 49: train loss 2.0272011480680328 val loss 1.5069981351501223\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 50: train loss 2.0570796146625425 val loss 1.4948352382969536\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 51: train loss 1.8855931177371885 val loss 1.4424782049502785\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 52: train loss 1.894168528114877 val loss 1.413936307013601\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 53: train loss 1.761890668694566 val loss 1.3934837970605187\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 54: train loss 1.6195537178981594 val loss 1.3517397843600478\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 55: train loss 1.566492109763913 val loss 1.3204763150293\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 56: train loss 1.5572230372487046 val loss 1.3064208482536896\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 57: train loss 1.4927020145625602 val loss 1.286917657765925\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 58: train loss 1.6331476220270482 val loss 1.3001399385776622\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 59: train loss 1.5122793731166095 val loss 1.2608927415562423\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 60: train loss 1.3764090123699932 val loss 1.232116960336105\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 61: train loss 1.5433264440152703 val loss 1.2692422512902626\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 62: train loss 1.4520816032479449 val loss 1.2326891996454614\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 63: train loss 1.421529208014651 val loss 1.2092793524170389\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 64: train loss 1.4646053786684827 val loss 1.2514184685255076\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 65: train loss 1.3400728549899124 val loss 1.180835325074654\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 66: train loss 1.5975595967071812 val loss 1.3624106818103974\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 67: train loss 1.5457200647854223 val loss 1.2314149571026938\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 68: train loss 2.7221536069381527 val loss 1.4077962261193555\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 69: train loss 1.986729143596277 val loss 1.2973359461040566\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 70: train loss 1.8173993484276096 val loss 1.2651645206691076\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 71: train loss 2.075792314075842 val loss 1.2892623754260042\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 72: train loss 1.582862591598092 val loss 1.2086123593854485\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 73: train loss 1.3725369630790338 val loss 1.1886774125088342\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 74: train loss 1.2912601254335263 val loss 1.1572737249884342\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 75: train loss 1.2249442033651399 val loss 1.1251343007312866\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 76: train loss 1.141079191754504 val loss 1.1170614413251145\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 77: train loss 1.1378331882197683 val loss 1.1044685072683775\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 78: train loss 1.1531891037778157 val loss 1.0782222655551132\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 79: train loss 1.1469719860611893 val loss 1.200638349130605\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 80: train loss 1.0805519056029436 val loss 1.0851073121528956\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 81: train loss 1.0197006703876867 val loss 1.047190018031595\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 82: train loss 1.0192269275828105 val loss 1.0341162131902228\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 83: train loss 1.1426769633118699 val loss 1.0421060442856942\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 84: train loss 1.2011219335765373 val loss 1.0530793958392626\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 85: train loss 1.1110315860771551 val loss 1.0139066550743143\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 86: train loss 1.0563772102681601 val loss 1.0015843920847995\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 87: train loss 1.0151781541545217 val loss 1.0110081857925246\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 88: train loss 0.971471608411975 val loss 0.9721429015108463\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 89: train loss 0.9379120185607817 val loss 0.9884617694634269\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 90: train loss 0.937274538162278 val loss 0.9800722585164255\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 91: train loss 1.2355712768508167 val loss 1.005210944900566\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 92: train loss 1.060197963947203 val loss 0.9766317795067768\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 93: train loss 0.9917084981755513 val loss 0.9723037513393574\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 94: train loss 0.9033635280481199 val loss 0.9595699439489561\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 95: train loss 0.8872661743222213 val loss 0.944975856108718\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 96: train loss 0.963369632639536 val loss 0.9670561368805187\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 97: train loss 1.151246396506705 val loss 0.985740005129559\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 98: train loss 0.9712230446862011 val loss 1.0074464500393552\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 99: train loss 0.9649011441847173 val loss 0.9509071964603272\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 100: train loss 0.9801275628369029 val loss 0.9493399295074839\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 101: train loss 0.8772716246000151 val loss 0.9413163881376546\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 102: train loss 0.8557407303554255 val loss 0.9476028759523483\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 103: train loss 0.8449152511794392 val loss 0.9179265168063191\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 104: train loss 0.8033388467823587 val loss 0.9039651013227962\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 105: train loss 0.7755216738072838 val loss 0.8956422428353183\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 106: train loss 0.7542738841801155 val loss 0.9108029966610959\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 107: train loss 0.7403075753188715 val loss 0.9071079390927738\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 108: train loss 0.7484895092685048 val loss 0.8778447775004934\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 109: train loss 0.7227067634826754 val loss 0.8883152885361701\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 110: train loss 0.7199861051105871 val loss 0.894381891580987\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 111: train loss 0.6927796634959011 val loss 0.8656850065066863\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 112: train loss 0.6695889877836879 val loss 0.8549379061329273\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 113: train loss 0.6623210656206783 val loss 0.850776834838657\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 114: train loss 0.6521076138426618 val loss 0.8567413397335217\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 115: train loss 0.660921037560556 val loss 0.8791636571735272\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 116: train loss 0.7944979238800887 val loss 1.0436205712102018\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 117: train loss 0.7966238180311714 val loss 0.8935536750176494\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 118: train loss 0.729005952433842 val loss 0.8899065996866049\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 119: train loss 0.7070527723649653 val loss 0.900101469584382\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 120: train loss 1.9316691036631422 val loss 1.6666342493754671\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 121: train loss 1.7329350004835826 val loss 1.0908954527356947\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 122: train loss 1.1519383482816743 val loss 0.9435499706848433\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 123: train loss 1.2644149624719851 val loss 0.9223789274555813\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 124: train loss 1.2060561674397166 val loss 0.8975884967728073\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 125: train loss 1.7074102686672676 val loss 1.2293135253358665\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 126: train loss 1.2200390900053628 val loss 0.9351532181217423\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 127: train loss 0.9991841083619653 val loss 0.9236039750713095\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 128: train loss 0.8784253757174422 val loss 0.8639517624605936\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 129: train loss 1.419153688884363 val loss 1.3460170815398012\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 130: train loss 1.452727966192292 val loss 1.2435540124478015\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 131: train loss 0.961512837468124 val loss 0.9156983030412269\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 132: train loss 0.8548565027190418 val loss 0.8778085864095954\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 133: train loss 0.9068587989341922 val loss 0.8413160423332398\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 134: train loss 0.7068738653892424 val loss 0.8299790607489459\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 135: train loss 0.6675077836687971 val loss 0.8204457093832856\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 136: train loss 0.6280180441170204 val loss 0.810230206777398\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 137: train loss 0.626532908256461 val loss 0.8065817129505888\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 138: train loss 0.7443265660506923 val loss 0.9584773141120292\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 139: train loss 0.7409991240356026 val loss 0.8387188988690312\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 140: train loss 0.7439905812100667 val loss 0.8138185799725447\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 141: train loss 0.8358702699585658 val loss 0.8065135478806507\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 142: train loss 0.7461174552033587 val loss 0.8107889558016508\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 143: train loss 2.6396042308429393 val loss 1.9741373822054407\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 144: train loss 1.8503481076984871 val loss 1.1477814759771197\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 145: train loss 1.1469710214835842 val loss 0.9649819402922498\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 146: train loss 0.9214114430474072 val loss 0.892725035810396\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 147: train loss 0.8203567382765979 val loss 0.8634094283272655\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 148: train loss 0.7884717314708524 val loss 0.8427922446747184\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 149: train loss 0.7499112417785133 val loss 0.8254980592894334\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 150: train loss 0.7218918033489367 val loss 0.8128467283243014\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 151: train loss 0.7310684097976219 val loss 0.8174490808876833\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 152: train loss 0.7473092598886024 val loss 0.8480793807883981\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 153: train loss 0.6343640089035034 val loss 0.8534670762248443\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 154: train loss 0.6678743431480919 val loss 0.8456636537405274\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 155: train loss 0.699262277745619 val loss 0.8717361239775523\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 156: train loss 0.5908489583469019 val loss 0.8456100958599733\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 157: train loss 0.5625571099723258 val loss 0.8680763118029757\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 158: train loss 0.5434957957122384 val loss 0.8657704941141829\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 159: train loss 0.530309886830609 val loss 0.864813858568781\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 160: train loss 0.5299609972209465 val loss 0.8756542078604188\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 161: train loss 0.5164746805662062 val loss 0.8519893225323849\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 162: train loss 0.504297872142094 val loss 0.8424958731453955\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 163: train loss 0.49933582362605305 val loss 0.868647523805023\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 164: train loss 0.5071219540223842 val loss 0.8479024951803837\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 165: train loss 0.490579226394979 val loss 0.83909133034706\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 166: train loss 0.4819213050167735 val loss 0.8478576779208309\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 167: train loss 0.4763631384547164 val loss 0.8119742190957935\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 168: train loss 0.49843212126231773 val loss 0.8563341365832375\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 169: train loss 0.5023059528775331 val loss 0.8467193426285261\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 170: train loss 0.5035188728716316 val loss 0.8344874350496427\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 171: train loss 0.49433159719153147 val loss 0.8431415092378987\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 172: train loss 0.4727809102070041 val loss 0.80725754455652\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 173: train loss 0.46366530874880346 val loss 0.8255175812140152\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 174: train loss 0.4589311625899338 val loss 0.7967080412445017\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 175: train loss 0.4536671253239236 val loss 0.8045505678600073\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 176: train loss 0.45305345952510834 val loss 0.8094698689478476\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 177: train loss 0.4505130804893447 val loss 0.8194171328408023\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 178: train loss 0.44479403081463603 val loss 0.8058618121394012\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 179: train loss 0.43977647656347696 val loss 0.8027400783191166\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 180: train loss 0.44472011196904065 val loss 0.795414611309347\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 181: train loss 0.44272590374074333 val loss 0.8114197641719271\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 182: train loss 0.43469074477509756 val loss 0.7909622773142461\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 183: train loss 0.433551929709388 val loss 0.8159596516836406\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 184: train loss 0.46579679556009246 val loss 0.8081453417643779\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 185: train loss 0.5704756684419585 val loss 0.8503377614713745\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 186: train loss 0.7297467200494394 val loss 1.0590208924941666\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 187: train loss 0.866950586801622 val loss 0.888425373628298\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 188: train loss 0.6181298806899931 val loss 0.8144453003369626\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 189: train loss 0.5344800076833586 val loss 0.7993834183576705\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 190: train loss 0.4958539746883439 val loss 0.8129984520089013\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 191: train loss 0.4724316444338822 val loss 0.7810018168437006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 192: train loss 0.43708623836680155 val loss 0.7444630935705719\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 193: train loss 0.518876141164361 val loss 0.8818675536028591\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 194: train loss 0.7499196445796548 val loss 0.8390153137724107\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 195: train loss 0.7317762876429209 val loss 0.7963535879048581\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 196: train loss 0.6637397572034742 val loss 0.7506569025600538\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 197: train loss 0.5534997691468495 val loss 0.7280956476338494\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 198: train loss 0.5083521688129844 val loss 0.7221246708764748\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 199: train loss 0.4803757914682714 val loss 0.7095248409452815\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 200: train loss 0.44177840541048746 val loss 0.6959148793454549\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 201: train loss 0.4143651051492226 val loss 0.7006107495202151\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 202: train loss 0.40192530104299873 val loss 0.7049494271452332\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 203: train loss 0.44142590063374215 val loss 0.7086537969361996\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 204: train loss 0.5634638445406426 val loss 0.7012751878953791\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 205: train loss 0.5075420359285866 val loss 0.711794013049114\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 206: train loss 0.4686714290845685 val loss 0.6988246595820361\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 207: train loss 0.41583228074922796 val loss 0.6976007739552867\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 208: train loss 0.37998726709586816 val loss 0.6872685225904656\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 209: train loss 0.3580499662131798 val loss 0.6814005530740107\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 210: train loss 0.34660860678044764 val loss 0.6790277298743151\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 211: train loss 0.34119865334615473 val loss 0.6786783180843567\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 212: train loss 0.3396383952803728 val loss 0.6780283495620744\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 213: train loss 0.33159394253317903 val loss 0.6804768393403893\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 214: train loss 0.32696867461611584 val loss 0.6879564014165734\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 215: train loss 0.3262275235681999 val loss 0.6790796330215535\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 216: train loss 0.3279684947394743 val loss 0.6822247872951831\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 217: train loss 0.3302840986629812 val loss 0.6919546091793561\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 218: train loss 0.33541116082086797 val loss 0.6873186956403459\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 219: train loss 0.350633747693969 val loss 0.6957568762711747\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 220: train loss 0.37045984751567607 val loss 0.7100612995711787\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 221: train loss 0.3764874800676253 val loss 0.6993013425383939\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 222: train loss 0.35722868144512177 val loss 0.7623886538545612\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 223: train loss 0.43524812816119773 val loss 0.6962183479612551\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 224: train loss 0.3705666523154189 val loss 0.6759017185058319\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 225: train loss 0.33789171750952557 val loss 0.6774940711381967\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 226: train loss 0.3340934236965528 val loss 0.6790891524124006\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 227: train loss 0.34374388870669575 val loss 0.6694362730552669\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 228: train loss 0.3900730284612353 val loss 0.7140662332434554\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 229: train loss 0.3584987197707339 val loss 0.682494898522329\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 230: train loss 0.3340077885403866 val loss 0.6753965083932719\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 231: train loss 0.30900913495116117 val loss 0.655287299893309\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 232: train loss 0.3040953022314281 val loss 0.6522416689353536\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 233: train loss 0.29325600749835734 val loss 0.6458492603125646\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 234: train loss 0.2842163723416445 val loss 0.6467990257327094\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 235: train loss 0.276604616605654 val loss 0.6433105365640005\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 236: train loss 0.27048613948792943 val loss 0.6377158771861756\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 237: train loss 0.269622245758045 val loss 0.6443597203183321\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 238: train loss 0.26636595351666936 val loss 0.6434070275848552\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 239: train loss 0.26769746730967264 val loss 0.636333382602609\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 240: train loss 0.27011063985708283 val loss 0.6419458660952159\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 241: train loss 0.2780060195704786 val loss 0.6688958022993585\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 242: train loss 0.2922921784040404 val loss 0.6620880844385567\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 243: train loss 0.2987480746900163 val loss 0.6556329160212139\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 244: train loss 0.2883540966161868 val loss 0.6477538806073438\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 245: train loss 0.2819633407563698 val loss 0.6412671096515375\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 246: train loss 0.2783156891421574 val loss 0.6404151765678708\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 247: train loss 0.28058776986308215 val loss 0.6440199679311643\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 248: train loss 0.2794628163299909 val loss 0.6550529559035521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 249: train loss 0.3124719621931634 val loss 0.6596532991040974\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 250: train loss 0.30736052899098976 val loss 0.6725601968264998\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 251: train loss 0.3224958855567909 val loss 0.6696837886078975\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 252: train loss 0.6931123097495335 val loss 0.8947685642649672\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 253: train loss 1.3246134595900048 val loss 0.9146945251518658\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 254: train loss 0.9386416880822763 val loss 0.7380363249729849\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 255: train loss 1.0354412179894563 val loss 0.8577603937704953\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 256: train loss 0.5870596114091757 val loss 0.6873996187755854\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 257: train loss 0.5286539135182776 val loss 0.6787026597129419\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 258: train loss 0.8407725942570988 val loss 0.777629617017948\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 259: train loss 0.5738393929673404 val loss 0.672290366212757\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 260: train loss 0.4659452785442515 val loss 0.6553113363999495\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 261: train loss 0.4218246785969269 val loss 0.6454005554792533\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 262: train loss 0.3832057376460331 val loss 0.6429227141231951\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 263: train loss 0.3526990062943319 val loss 0.6350226521991444\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 264: train loss 0.32888791055941 val loss 0.6325846575589614\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 265: train loss 0.31056198495917203 val loss 0.6281344845890697\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 266: train loss 0.2939032932243696 val loss 0.6269678387632883\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 267: train loss 0.31471698476773935 val loss 0.6359441271701197\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 268: train loss 0.2802533134818077 val loss 0.6279090979069774\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 269: train loss 0.25986380130052567 val loss 0.620865005717612\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 270: train loss 0.24830192545565163 val loss 0.6185223736937178\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 271: train loss 0.23820187769285062 val loss 0.616126699562125\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 272: train loss 0.22923535381148502 val loss 0.6159830245491265\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 273: train loss 0.2251781421463664 val loss 0.6148519851446284\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 274: train loss 0.24127114546008227 val loss 0.6111983009240577\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 275: train loss 0.23183135161312615 val loss 0.6148898284411317\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 276: train loss 0.22489937085930894 val loss 0.6254360801706191\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 277: train loss 0.21831539091540547 val loss 0.6155589302246871\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 278: train loss 0.21284098432558338 val loss 0.6169820455909668\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 279: train loss 0.21042935114081313 val loss 0.627794714121185\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 280: train loss 0.21489649393209598 val loss 0.6822722684446239\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 281: train loss 0.2777074057517982 val loss 0.6621544100784509\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 282: train loss 0.2648311569923308 val loss 0.6537358050731442\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 283: train loss 0.2559562007465014 val loss 0.6491032126342551\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 284: train loss 0.23411407139970036 val loss 0.625732669052676\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 285: train loss 0.22043569813050876 val loss 0.6158934874839708\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 286: train loss 0.2119475370500146 val loss 0.6064354560783516\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 287: train loss 0.2072608487271681 val loss 0.6086756277162685\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 288: train loss 0.2004495464810511 val loss 0.6079496841121502\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 289: train loss 0.19683386149202906 val loss 0.6028516169073259\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 290: train loss 0.19417457409748218 val loss 0.603808272802287\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 291: train loss 0.19471479552548107 val loss 0.6025618130199624\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 292: train loss 0.19464137496017828 val loss 0.602245327051478\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 293: train loss 0.19704614152632108 val loss 0.6091712426287862\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 294: train loss 0.19696350368421253 val loss 0.6204101725243996\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 295: train loss 0.1934769699668012 val loss 0.6027141555649951\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 296: train loss 0.1924679375639776 val loss 0.5989693114906992\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 297: train loss 0.19946885145292048 val loss 0.612204368876467\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 298: train loss 0.1998283243397387 val loss 0.6000376959140927\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 299: train loss 0.3993844916907752 val loss 1.3579239169471866\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 300: train loss 0.37419750795858664 val loss 0.6544598103295819\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 301: train loss 0.3452221525878441 val loss 0.6664742882798196\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 302: train loss 0.3106106089019194 val loss 0.6083623775733805\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 303: train loss 0.2542857152477997 val loss 0.5974647068460384\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 304: train loss 0.2102578796628045 val loss 0.5962957428831209\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 305: train loss 0.19090818213980373 val loss 0.5972403294619153\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 306: train loss 0.18980355133734098 val loss 0.592348175361342\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 307: train loss 0.18836086497801105 val loss 0.6035439321060105\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 308: train loss 0.1915392613083851 val loss 0.5927096691758381\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 309: train loss 0.18362768521396125 val loss 0.5859195762813544\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 310: train loss 0.17747483865880384 val loss 0.595391864188169\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 311: train loss 0.18656785577172186 val loss 0.6030550166814337\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 312: train loss 0.1837805714912531 val loss 0.5900373040577932\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 313: train loss 0.17494269996518041 val loss 0.593022113789793\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 314: train loss 0.17338113695746515 val loss 0.5850842331014557\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 315: train loss 0.17116550029051014 val loss 0.607618980840253\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 316: train loss 0.16404893685404848 val loss 0.5988068882423607\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 317: train loss 0.1637547909304863 val loss 0.5972174336339166\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 318: train loss 0.1659253510396655 val loss 0.6076120962086214\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 319: train loss 0.1666933372616768 val loss 0.5937125112908443\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 320: train loss 0.1715411943633382 val loss 0.6161896506326823\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 321: train loss 0.1778449519741826 val loss 0.605987301352273\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 322: train loss 0.17798611512634813 val loss 0.59959971357673\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 323: train loss 0.17267018565680922 val loss 0.5962984772303161\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 324: train loss 0.1627139109300404 val loss 0.5893430888876182\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 325: train loss 0.1575979572243807 val loss 0.5845869524835284\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 326: train loss 0.15754858940476324 val loss 0.5843641942394905\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 327: train loss 0.16078851372003555 val loss 0.5860311429332765\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 328: train loss 0.16636709042075204 val loss 0.5917800207540316\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 329: train loss 0.1709201265035606 val loss 0.5961460465304345\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 330: train loss 0.16555839976886425 val loss 0.6069259359176586\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 331: train loss 0.16475267871850874 val loss 0.6026072339931887\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 332: train loss 0.16408659535937192 val loss 0.5927803651218597\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 333: train loss 0.1671363789497352 val loss 0.5939395076250223\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 334: train loss 0.16873997985953237 val loss 0.6153665078647671\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 335: train loss 0.1778449126314826 val loss 0.5941997657910822\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 336: train loss 0.19442681922781757 val loss 0.6585148322992734\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 337: train loss 0.2422131632522839 val loss 0.6344072668604466\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 338: train loss 0.3396021340314935 val loss 0.8313125384599935\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 339: train loss 0.3396496611033998 val loss 0.6390369429063238\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 340: train loss 0.24215801296437658 val loss 0.587769018224258\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 341: train loss 0.5093513899823514 val loss 0.6333852410061117\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 342: train loss 0.36229247545323723 val loss 0.5895629291124489\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 343: train loss 0.292709354947253 val loss 0.5748291644953362\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 344: train loss 0.9897147806861052 val loss 0.7952913002891945\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 345: train loss 0.5538358146824488 val loss 0.6256694920328469\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 346: train loss 0.3494924014297927 val loss 0.5929207897261316\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 347: train loss 0.2987046803279621 val loss 0.5748166575517071\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 348: train loss 0.2675817199959988 val loss 0.5781957970592632\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 349: train loss 0.22114739448922435 val loss 0.5740674830140648\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 350: train loss 0.22099066325804081 val loss 0.5714161916176401\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 351: train loss 0.31578326961252745 val loss 0.5962683202532749\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 352: train loss 0.33437716679238694 val loss 0.5781188569088858\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 353: train loss 0.1892339876148759 val loss 0.5752879891938463\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 354: train loss 0.20634097778579083 val loss 0.5604226704435954\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 355: train loss 0.18097885171087777 val loss 0.5600233988571306\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 356: train loss 0.15898036366192306 val loss 0.5590501082586088\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 357: train loss 0.14870879835471873 val loss 0.5653436191025952\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 358: train loss 0.15283592336061524 val loss 0.5610601376623598\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 359: train loss 0.13001207648435745 val loss 0.5678426705955568\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 360: train loss 0.12092393941086967 val loss 0.5691858629635451\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 361: train loss 0.1181351591991942 val loss 0.5825312139212764\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 362: train loss 0.11881408127161061 val loss 0.5739041304579009\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 363: train loss 0.11643026410261305 val loss 0.5614156283611924\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 364: train loss 0.11661907639808772 val loss 0.5614474537450718\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 365: train loss 0.1148573618563937 val loss 0.5613171919453579\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 366: train loss 0.11383192495601933 val loss 0.5703711875135603\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 367: train loss 0.11432288082816251 val loss 0.5530705929680833\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 368: train loss 0.11237705031000986 val loss 0.5561842048350305\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 369: train loss 0.1114168373822439 val loss 0.569156976055146\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 370: train loss 0.11095461521933718 val loss 0.5668649938351508\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 371: train loss 0.11127524486765629 val loss 0.5626820822673932\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 372: train loss 0.11152565969926555 val loss 0.5662236366413343\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 373: train loss 0.11213266104459763 val loss 0.5619956209478214\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 374: train loss 0.11334682237811206 val loss 0.5590311356044365\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 375: train loss 0.11267534334485124 val loss 0.563021638879226\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 376: train loss 0.11416078208968407 val loss 0.5730836961991117\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 377: train loss 0.11418878014494734 val loss 0.5637989876516781\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 378: train loss 0.11307813808685396 val loss 0.5602339926331698\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 379: train loss 0.1406360663837049 val loss 0.646496273287683\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 380: train loss 0.22749640174755237 val loss 0.6155660955278791\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 381: train loss 0.17789865176125272 val loss 0.5897462530987898\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 382: train loss 0.14159197451137914 val loss 0.573074322725873\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 383: train loss 0.1270264781920648 val loss 0.5599456731673264\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 384: train loss 0.11625122615113491 val loss 0.5686534151864872\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 385: train loss 0.12055358417877336 val loss 0.5493291783652132\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 386: train loss 0.10937034302368397 val loss 0.550865708933916\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 387: train loss 0.11024503936854804 val loss 0.5544603626273025\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 388: train loss 0.13035855805728494 val loss 0.5731646368779795\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 389: train loss 0.11879085758473815 val loss 0.5612706206768605\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 390: train loss 0.11648043435884685 val loss 0.5737997349355666\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 391: train loss 0.12680502426696988 val loss 0.5624312360855246\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 392: train loss 0.2688078776728816 val loss 0.5824100877148692\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 393: train loss 0.22199175806670654 val loss 0.6295906868616492\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 394: train loss 0.28144264475601477 val loss 0.6825550728866996\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 395: train loss 0.19327200976450268 val loss 0.6635612768361032\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 396: train loss 0.1712634232712955 val loss 0.5635665538583561\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 397: train loss 0.13930792234292844 val loss 0.5664726572569371\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 398: train loss 0.11715283222133066 val loss 0.5547418897956029\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 399: train loss 0.10417295983288347 val loss 0.5551662106593828\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 400: train loss 0.09953569598132517 val loss 0.5519098464556104\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 401: train loss 0.09237895233602059 val loss 0.5499794415026056\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 402: train loss 0.09034248518689376 val loss 0.5485545886752213\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 403: train loss 0.09348654856042164 val loss 0.558686809772713\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 404: train loss 0.09367990879932554 val loss 0.5554234095486078\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 405: train loss 0.09626836306983377 val loss 0.5589600359678787\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 406: train loss 0.10134464425102967 val loss 0.5495888938561736\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 407: train loss 0.10698747325961183 val loss 0.5526821454759729\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 408: train loss 0.1091751130524932 val loss 0.5775481866574014\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 409: train loss 0.10973219346345925 val loss 0.5817467661922623\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 410: train loss 0.16825209285427883 val loss 0.6462268549291456\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 411: train loss 0.21291077264198444 val loss 0.6779315725031441\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 412: train loss 0.1935733849319016 val loss 0.572872966576596\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 413: train loss 0.15222695170015824 val loss 0.5656707814714866\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 414: train loss 0.12683472368957066 val loss 0.5643285207004232\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 415: train loss 0.11482166062767912 val loss 0.5598792719539831\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 416: train loss 0.10231936005194013 val loss 0.5466958397580391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 417: train loss 0.10087545815764404 val loss 0.5622326332428038\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 418: train loss 0.09880720328812193 val loss 0.5536021843121008\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 419: train loss 0.09264722121198003 val loss 0.5496627282362213\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 420: train loss 0.09128958845465648 val loss 0.5465091423932057\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 421: train loss 0.09110997599072573 val loss 0.546475069544685\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 422: train loss 0.09357581224019934 val loss 0.5528143466310435\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 423: train loss 0.09477877276154553 val loss 0.5487445915642735\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 424: train loss 0.09777316151232254 val loss 0.5471172557601066\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 425: train loss 0.10049423311905163 val loss 0.5568006328489657\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 426: train loss 0.10297982521900316 val loss 0.567078708532853\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 427: train loss 0.10976929399298459 val loss 0.567588359112984\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 428: train loss 0.11358015584509547 val loss 0.5556904192018043\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 429: train loss 0.11238374651932134 val loss 0.5671192463566807\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 430: train loss 0.1043372375936043 val loss 0.5606192517425465\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 431: train loss 0.09742222358359069 val loss 0.5507271092884997\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 432: train loss 0.09339463874334242 val loss 0.5399356124506836\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 433: train loss 0.09102008628045641 val loss 0.5412696970487777\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 434: train loss 0.09088728095336658 val loss 0.5395561746712283\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 435: train loss 0.09042150686245139 val loss 0.5509245862979449\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 436: train loss 0.09019059760541451 val loss 0.5463408609889995\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 437: train loss 0.09232156373923872 val loss 0.5476554903693961\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 438: train loss 0.09529239320900382 val loss 0.5595535782752125\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 439: train loss 0.09740456916028406 val loss 0.5792972924441521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 440: train loss 0.10255686402684305 val loss 0.5572024245524461\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 441: train loss 0.10147359367550873 val loss 0.565976626343767\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 442: train loss 0.09715867242435129 val loss 0.5597834741082066\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 443: train loss 0.09054002297542445 val loss 0.5479982250837757\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 444: train loss 0.08592102385875655 val loss 0.5475077274283\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 445: train loss 0.08612903875366944 val loss 0.5554175202023811\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 446: train loss 0.09486922181052405 val loss 0.5569580493880205\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 447: train loss 0.24485058460112025 val loss 0.6851970187918361\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 448: train loss 0.27597713524975426 val loss 0.8056690492635088\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 449: train loss 0.44041105014521903 val loss 0.6300329279837348\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 450: train loss 0.35202264131569283 val loss 0.5958879543994136\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 451: train loss 0.2183428970051975 val loss 0.5663844861238165\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 452: train loss 0.1410189788879418 val loss 0.5441196634139004\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 453: train loss 0.1196506353231465 val loss 0.5387266029255237\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 454: train loss 0.09543670418604118 val loss 0.5383420470204857\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 455: train loss 0.08330267345214762 val loss 0.530318241441057\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 456: train loss 0.07418654954469786 val loss 0.5284526257287443\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 457: train loss 0.06936635122430033 val loss 0.527859153076377\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 458: train loss 0.06605251153885591 val loss 0.5260559166455129\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 459: train loss 0.0637277263421111 val loss 0.5250531144805656\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 460: train loss 0.061772745034498415 val loss 0.5265768184110446\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 461: train loss 0.06045469959698072 val loss 0.5269452124377246\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 462: train loss 0.05967215241909754 val loss 0.529544516088359\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 463: train loss 0.05947809541461671 val loss 0.530392239812154\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 464: train loss 0.06039120915641145 val loss 0.5345871496638949\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 465: train loss 0.06086399031394139 val loss 0.5292238668758391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 466: train loss 0.06196075596097039 val loss 0.5465655051327503\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 467: train loss 0.06254781242005708 val loss 0.5346136374778773\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 468: train loss 0.06273614051865369 val loss 0.5376417973070594\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 469: train loss 0.06622044726206762 val loss 0.5483509223963579\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 470: train loss 0.0695526190783556 val loss 0.5431135685085039\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 471: train loss 0.07177605027923496 val loss 0.5678207784029725\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 472: train loss 0.07697541205348765 val loss 0.5463953801775685\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 473: train loss 0.07314935412893935 val loss 0.5463040422063652\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 474: train loss 0.06900771223462937 val loss 0.5429327536542168\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 475: train loss 0.06449803273852278 val loss 0.5385343284472179\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 476: train loss 0.06287591193416496 val loss 0.5282116043534955\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 477: train loss 0.060834512455252614 val loss 0.5326813479619754\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 478: train loss 0.060424571402552654 val loss 0.5339919111890418\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 479: train loss 0.060615753777688595 val loss 0.5363362505501688\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 480: train loss 0.06176153154725709 val loss 0.5377886148022559\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 481: train loss 0.06391393411450269 val loss 0.5311321876113728\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 482: train loss 0.06430018965790911 val loss 0.5326259983488859\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 483: train loss 0.06618862744511628 val loss 0.5307026821691687\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 484: train loss 0.07048087730640318 val loss 0.5410367563839488\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 485: train loss 0.07234846346262025 val loss 0.5433396724339644\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 486: train loss 0.06909387240686067 val loss 0.5413557850218763\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 487: train loss 0.06646687694166492 val loss 0.542613187187656\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 488: train loss 0.06615489787172253 val loss 0.5423677265746113\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 489: train loss 0.06741573445771526 val loss 0.5453061243853051\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 490: train loss 0.07052212503806847 val loss 0.5386145421287121\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 491: train loss 0.07193558045276781 val loss 0.5518004093736886\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 492: train loss 0.07131986070151736 val loss 0.5355912941705138\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 493: train loss 0.07244761928734256 val loss 0.5444341436170129\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 494: train loss 0.12688431938792147 val loss 0.6060551007409759\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 495: train loss 0.1887259489697654 val loss 0.5804336214897912\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 496: train loss 0.14831563512363086 val loss 0.5624832945905178\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 497: train loss 0.1384117941303951 val loss 0.5402752471896128\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 498: train loss 0.09968495046401896 val loss 0.5330083167988893\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 499: train loss 0.08919354555446928 val loss 0.5894425096938714\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 500: train loss 0.11994826770955469 val loss 0.544664396529628\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 501: train loss 0.07485182692364949 val loss 0.5329089525042324\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 502: train loss 0.05988390375746459 val loss 0.5264302184547913\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 503: train loss 0.05784451284604829 val loss 0.5283431936202599\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 504: train loss 0.05746347563931855 val loss 0.5182684333327959\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 505: train loss 0.05188933271551278 val loss 0.5166195547721403\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 506: train loss 0.04901220630172912 val loss 0.5216655804971089\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 507: train loss 0.04769080825059152 val loss 0.5209792132616756\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 508: train loss 0.0475740310304412 val loss 0.5199250174088\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 509: train loss 0.048252318940329844 val loss 0.5246554042958558\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 510: train loss 0.05607015026233545 val loss 0.5387543691971137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 511: train loss 0.06942557984190743 val loss 0.5397724621974065\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 512: train loss 0.07428745059977944 val loss 0.5436414869216979\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 513: train loss 0.0801079161465168 val loss 0.5801396762554305\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 514: train loss 0.11289586381214421 val loss 0.5517525147694519\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 515: train loss 0.8545838330940503 val loss 0.868560589603446\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 516: train loss 0.7123108133673668 val loss 0.6708557351889646\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 517: train loss 0.4662114409048383 val loss 0.6341158612326652\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 518: train loss 0.42278828362866144 val loss 0.6348563205186979\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 519: train loss 0.5589043356296493 val loss 0.7413941295873792\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 520: train loss 0.3636359198064339 val loss 0.6124016302518124\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 521: train loss 0.22266335576409246 val loss 0.5765686310714735\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 522: train loss 0.15758218875200283 val loss 0.5653648804307739\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 523: train loss 0.13690436390660157 val loss 0.5553265534599601\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 524: train loss 0.12145194370390439 val loss 0.5505758177631093\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 525: train loss 0.11725447853890861 val loss 0.5545086170826087\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 526: train loss 0.11488584015609288 val loss 0.5404359031696233\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 527: train loss 0.1003682102281146 val loss 0.5404158902139602\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 528: train loss 0.09231642431511385 val loss 0.5395638892064017\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 529: train loss 0.08660241186891388 val loss 0.5372749173830315\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 530: train loss 0.08186261960100837 val loss 0.5362510378589124\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 531: train loss 0.07888946205195857 val loss 0.5356454552281175\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 532: train loss 0.07421065210478335 val loss 0.5347565139117265\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 533: train loss 0.07025618867085474 val loss 0.5338138233610293\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 534: train loss 0.06662734529775817 val loss 0.5327097873430104\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 535: train loss 0.06354955608797509 val loss 0.532631453500721\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 536: train loss 0.06084887928715566 val loss 0.531860890700127\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 537: train loss 0.05873758909178943 val loss 0.5304154294252131\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 538: train loss 0.056813209045042355 val loss 0.531395774942532\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 539: train loss 0.05563881427685662 val loss 0.5295028597398611\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 540: train loss 0.054289361280275554 val loss 0.532530129912221\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 541: train loss 0.054198179098709325 val loss 0.5296276530633127\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 542: train loss 0.05336703235147203 val loss 0.5308860120785414\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 543: train loss 0.052110926270848366 val loss 0.529810319692745\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 544: train loss 0.05157667848213417 val loss 0.5305501779912384\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 545: train loss 0.052200235380995566 val loss 0.5313246304230923\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 546: train loss 0.052176611020979355 val loss 0.5307767956398457\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 547: train loss 0.05243836872552226 val loss 0.5353892355637706\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 548: train loss 0.05308066124505386 val loss 0.5333919496476447\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 549: train loss 0.05387543793767691 val loss 0.5289548230860125\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 550: train loss 0.0525413770682928 val loss 0.530349556012449\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 45 \n",
      "Epoch 551: train loss 0.051472032079245986 val loss 0.5286022696587469\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 46 \n",
      "Epoch 552: train loss 0.05051447108115365 val loss 0.5246767115716691\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 47 \n",
      "Epoch 553: train loss 0.04983281507724669 val loss 0.5240504343789683\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 48 \n",
      "Epoch 554: train loss 0.049198601485752474 val loss 0.5278207837514363\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 49 \n",
      "Epoch 555: train loss 0.04848724224309369 val loss 0.5233734002990214\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 50 \n",
      "Epoch 556: train loss 0.047536357169652856 val loss 0.5252683065648543\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 51 \n",
      "Epoch 557: train loss 0.0470855083861729 val loss 0.5258778428119442\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 52 \n",
      "Epoch 558: train loss 0.04693388934360772 val loss 0.5225827211496221\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 53 \n",
      "Epoch 559: train loss 0.04710151391421876 val loss 0.5260268823125152\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 54 \n",
      "Epoch 560: train loss 0.04769900906831026 val loss 0.5287633308936622\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 55 \n",
      "Epoch 561: train loss 0.048467683810286405 val loss 0.5294094467676359\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 56 \n",
      "Epoch 562: train loss 0.048938098340863136 val loss 0.5282273311602251\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 57 \n",
      "Epoch 563: train loss 0.05033180801334178 val loss 0.5353995748304071\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 58 \n",
      "Epoch 564: train loss 0.050799509419537175 val loss 0.5473682153755368\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 59 \n",
      "Epoch 565: train loss 0.05217668166520392 val loss 0.5346656324412667\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 60 \n",
      "Epoch 566: train loss 0.05228657057372535 val loss 0.5307272933629106\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 61 \n",
      "Epoch 567: train loss 0.05293589444240419 val loss 0.5409975028744097\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 62 \n",
      "Epoch 568: train loss 0.05338432439943639 val loss 0.5315479014539027\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 63 \n",
      "Epoch 569: train loss 0.053357558279502684 val loss 0.5266792121519647\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 64 \n",
      "Epoch 570: train loss 0.05349039177342159 val loss 0.5439457277513418\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 65 \n",
      "Epoch 571: train loss 0.05691907223223186 val loss 0.5470944406005871\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 66 \n",
      "Epoch 572: train loss 0.056963472740679255 val loss 0.5408909502417587\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 67 \n",
      "Epoch 573: train loss 0.05725079811200863 val loss 0.5336909179586247\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 68 \n",
      "Epoch 574: train loss 0.05515964036216823 val loss 0.5299614446287144\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 69 \n",
      "Epoch 575: train loss 0.0798413824153746 val loss 0.5800588715088429\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 70 \n",
      "Epoch 576: train loss 0.11827705759645962 val loss 0.5705829242060692\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 71 \n",
      "Epoch 577: train loss 0.23054929445611266 val loss 0.7659139087623137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 72 \n",
      "Epoch 578: train loss 0.2991352267563343 val loss 0.603312168304128\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 73 \n",
      "Epoch 579: train loss 0.17007152409088322 val loss 0.5708920338284788\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 74 \n",
      "Epoch 580: train loss 0.11894571190563644 val loss 0.5314360055111393\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 75 \n",
      "Epoch 581: train loss 0.1280032008341173 val loss 0.5453840078946766\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 76 \n",
      "Epoch 582: train loss 0.10352412422710075 val loss 0.5240448392210204\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 77 \n",
      "Epoch 583: train loss 0.08655484436397873 val loss 0.5222493273414708\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 78 \n",
      "Epoch 584: train loss 0.07717655245896156 val loss 0.5186676571374563\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 79 \n",
      "Epoch 585: train loss 0.07114735111685061 val loss 0.5223590166352521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 80 \n",
      "Epoch 586: train loss 0.06530542263942885 val loss 0.5196068686971005\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 81 \n",
      "Epoch 587: train loss 0.06242951127223489 val loss 0.5171995976397079\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 82 \n",
      "Epoch 588: train loss 0.057864922791628574 val loss 0.5163567719912663\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 589: train loss 0.04946375330455783 val loss 0.5152992023970051\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 590: train loss 0.04602510538860792 val loss 0.5138326269835695\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 591: train loss 0.043739023243599545 val loss 0.5127434415915116\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 592: train loss 0.04095870609645073 val loss 0.5092170768917084\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 593: train loss 0.0385449001247563 val loss 0.5090181756346591\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 594: train loss 0.03741100940444484 val loss 0.5085943165502484\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 0 \n",
      "Epoch 595: train loss 0.04040202348515755 val loss 0.5175370219872745\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 1 \n",
      "Epoch 596: train loss 0.05858124017988036 val loss 0.5130956010363288\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 2 \n",
      "Epoch 597: train loss 0.05199223704545236 val loss 0.5144642990597524\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 3 \n",
      "Epoch 598: train loss 0.051898042330654655 val loss 0.5123891769959137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 4 \n",
      "Epoch 599: train loss 0.056355794346550615 val loss 0.5203420965737178\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 5 \n",
      "Epoch 600: train loss 0.054484617396643974 val loss 0.5095385220275526\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 6 \n",
      "Epoch 601: train loss 0.05125981310337055 val loss 0.5191133604709424\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 7 \n",
      "Epoch 602: train loss 0.05405498573147669 val loss 0.5132204239550332\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 8 \n",
      "Epoch 603: train loss 0.04909235593385813 val loss 0.5114474353103637\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 9 \n",
      "Epoch 604: train loss 0.0455753192441856 val loss 0.5161835856505288\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 10 \n",
      "Epoch 605: train loss 0.04453226796737531 val loss 0.5151836097890685\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 11 \n",
      "Epoch 606: train loss 0.04526725559063801 val loss 0.5174162535483992\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 12 \n",
      "Epoch 607: train loss 0.04742322117090225 val loss 0.5145611888368631\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 13 \n",
      "Epoch 608: train loss 0.049917247078222475 val loss 0.5207516821933958\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 14 \n",
      "Epoch 609: train loss 0.050711190759590484 val loss 0.5244904713175784\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 15 \n",
      "Epoch 610: train loss 0.051326204432038273 val loss 0.5182252868188932\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 16 \n",
      "Epoch 611: train loss 0.053859039641371585 val loss 0.5230073769774793\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 17 \n",
      "Epoch 612: train loss 0.055980629832824556 val loss 0.538962645388286\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 18 \n",
      "Epoch 613: train loss 0.056487048930692965 val loss 0.5284445672305538\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 19 \n",
      "Epoch 614: train loss 0.05434361477268905 val loss 0.5196092777746965\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 20 \n",
      "Epoch 615: train loss 0.04971551254573392 val loss 0.5169895550464363\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 21 \n",
      "Epoch 616: train loss 0.06129716509362546 val loss 0.5472501896525148\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 22 \n",
      "Epoch 617: train loss 0.07659768199593556 val loss 0.5333711921130544\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 23 \n",
      "Epoch 618: train loss 0.08934724439934986 val loss 0.5773117852383993\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 24 \n",
      "Epoch 619: train loss 0.10836005260849871 val loss 0.5520963163747628\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 25 \n",
      "Epoch 620: train loss 0.08591169459608997 val loss 0.5352738264186906\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 26 \n",
      "Epoch 621: train loss 0.07525500477995814 val loss 0.5320347600930374\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 27 \n",
      "Epoch 622: train loss 0.07100645143811296 val loss 0.5311099970246792\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 28 \n",
      "Epoch 623: train loss 0.055121327977536654 val loss 0.5250952798214052\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 29 \n",
      "Epoch 624: train loss 0.04552829320110926 val loss 0.5208091413704674\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 30 \n",
      "Epoch 625: train loss 0.04004148968563574 val loss 0.5096529955790331\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 31 \n",
      "Epoch 626: train loss 0.03762723374884666 val loss 0.5088781616796041\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 32 \n",
      "Epoch 627: train loss 0.036652885093467265 val loss 0.5150644509990115\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 33 \n",
      "Epoch 628: train loss 0.03630865081326991 val loss 0.5184057810281875\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 34 \n",
      "Epoch 629: train loss 0.03729635723525795 val loss 0.5271124097658915\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 35 \n",
      "Epoch 630: train loss 0.038551492684679785 val loss 0.530141531271272\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 36 \n",
      "Epoch 631: train loss 0.03955070285989744 val loss 0.5272362142013521\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 37 \n",
      "Epoch 632: train loss 0.04190578000483716 val loss 0.5189255470718269\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 38 \n",
      "Epoch 633: train loss 0.04524485756665832 val loss 0.5254594691530825\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 39 \n",
      "Epoch 634: train loss 0.04947591211828517 val loss 0.5288580493867399\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 40 \n",
      "Epoch 635: train loss 0.05791734406588281 val loss 0.5462012941266361\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 41 \n",
      "Epoch 636: train loss 0.07739890493997713 val loss 0.5644169026799882\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 42 \n",
      "Epoch 637: train loss 0.16066916686732594 val loss 0.6167566748429955\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 43 \n",
      "Epoch 638: train loss 0.1180701961331978 val loss 0.5752021173676833\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 44 \n",
      "Epoch 639: train loss 0.09201530080924673 val loss 0.5566079106782233\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 45 \n",
      "Epoch 640: train loss 0.0679350761787557 val loss 0.5404791701204543\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 46 \n",
      "Epoch 641: train loss 0.052504606408680356 val loss 0.5385288268980715\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 47 \n",
      "Epoch 642: train loss 0.045402610424633436 val loss 0.529476497775265\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 48 \n",
      "Epoch 643: train loss 0.04161922878971914 val loss 0.5358621875354193\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 49 \n",
      "Epoch 644: train loss 0.04019526767021999 val loss 0.5323144615083599\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 50 \n",
      "Epoch 645: train loss 0.03902545432764583 val loss 0.5309517630838114\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 51 \n",
      "Epoch 646: train loss 0.039015041800533855 val loss 0.5253071763789426\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 52 \n",
      "Epoch 647: train loss 0.03892642320928777 val loss 0.523222905043399\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 53 \n",
      "Epoch 648: train loss 0.03931119915370534 val loss 0.5186937557703684\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 54 \n",
      "Epoch 649: train loss 0.03965544684721929 val loss 0.5262515095962259\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 55 \n",
      "Epoch 650: train loss 0.040440235945691426 val loss 0.5333998559373258\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 56 \n",
      "Epoch 651: train loss 0.042787440166604226 val loss 0.5343927381681749\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 57 \n",
      "Epoch 652: train loss 0.04686911154265811 val loss 0.5256701191534997\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 58 \n",
      "Epoch 653: train loss 0.05146070551581499 val loss 0.5274950052413835\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 59 \n",
      "Epoch 654: train loss 0.05195190848373785 val loss 0.5322240974251137\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 60 \n",
      "Epoch 655: train loss 0.0512189205009036 val loss 0.5236580835633516\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 61 \n",
      "Epoch 656: train loss 0.04979847192128257 val loss 0.5239678232948244\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 62 \n",
      "Epoch 657: train loss 0.04639222763660478 val loss 0.5201889662088549\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 63 \n",
      "Epoch 658: train loss 0.04594760981002232 val loss 0.5301407565075382\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 64 \n",
      "Epoch 659: train loss 0.04677902078028859 val loss 0.5300472303979672\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 65 \n",
      "Epoch 660: train loss 0.047447427902824996 val loss 0.5310467781426595\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 66 \n",
      "Epoch 661: train loss 0.16326666182679375 val loss 0.6259180784011533\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 67 \n",
      "Epoch 662: train loss 0.19708602802782524 val loss 0.5634889357029831\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 68 \n",
      "Epoch 663: train loss 0.12079053508435808 val loss 0.5479513361886391\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 69 \n",
      "Epoch 664: train loss 0.08976945748961554 val loss 0.5346304104573054\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 70 \n",
      "Epoch 665: train loss 0.06031177356475737 val loss 0.525555593082448\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 71 \n",
      "Epoch 666: train loss 0.045727693748365085 val loss 0.5276777625114284\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 72 \n",
      "Epoch 667: train loss 0.039243640548481444 val loss 0.5229213025859422\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 73 \n",
      "Epoch 668: train loss 0.03504436904927943 val loss 0.5237746323345853\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 74 \n",
      "Epoch 669: train loss 0.03325361217849138 val loss 0.5252109346888691\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 75 \n",
      "Epoch 670: train loss 0.032432271485648506 val loss 0.5239860426017992\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 76 \n",
      "Epoch 671: train loss 0.03195198031324076 val loss 0.518318170434917\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 77 \n",
      "Epoch 672: train loss 0.031135814468853357 val loss 0.5253534813486983\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 78 \n",
      "Epoch 673: train loss 0.031080713836339917 val loss 0.529569606338891\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 79 \n",
      "Epoch 674: train loss 0.031634953418155996 val loss 0.5281807712759247\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 80 \n",
      "Epoch 675: train loss 0.03185484394794557 val loss 0.5305778431476014\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 81 \n",
      "Epoch 676: train loss 0.0333322475596172 val loss 0.5206821640125143\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 82 \n",
      "Epoch 677: train loss 0.033052958911511957 val loss 0.5241133638734335\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 83 \n",
      "Epoch 678: train loss 0.03334878724658998 val loss 0.5234801159027054\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 84 \n",
      "Epoch 679: train loss 0.0341096228760917 val loss 0.5227942554516561\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 85 \n",
      "Epoch 680: train loss 0.03437352865343777 val loss 0.5231458306033234\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 86 \n",
      "Epoch 681: train loss 0.03495337147401964 val loss 0.5252293224548967\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 87 \n",
      "Epoch 682: train loss 0.036277169373068144 val loss 0.5223640594503507\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 88 \n",
      "Epoch 683: train loss 0.03641794879761774 val loss 0.5284943344428494\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 89 \n",
      "Epoch 684: train loss 0.03794674623030715 val loss 0.529087807301683\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 90 \n",
      "Epoch 685: train loss 0.038599297722301834 val loss 0.5192315802336757\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 91 \n",
      "Epoch 686: train loss 0.03923110977360388 val loss 0.5193340971054379\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 92 \n",
      "Epoch 687: train loss 0.038295920506664895 val loss 0.5162581294087224\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 93 \n",
      "Epoch 688: train loss 0.03769588422757096 val loss 0.5216031243715199\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 94 \n",
      "Epoch 689: train loss 0.03672087331098027 val loss 0.5228906827921487\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 95 \n",
      "Epoch 690: train loss 0.03582674228563541 val loss 0.5238002467441488\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 96 \n",
      "Epoch 691: train loss 0.034795343966745745 val loss 0.5186446976168053\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 97 \n",
      "Epoch 692: train loss 0.034695379390585715 val loss 0.5181423176348106\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 98 \n",
      "Epoch 693: train loss 0.03501179231694195 val loss 0.5176665324365448\n",
      "Current learning rate: 0.001\n",
      "early_stopping_counter: 99 \n",
      "Epoch 694: train loss 0.03580667615709145 val loss 0.5151319381817767\n",
      "Current learning rate: 0.001\n",
      "Early stopping at epoch 694 due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model = model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for seq_true, _, _, _ in data_loaders['trn']:\n",
    "        seq_true = seq_true.to(device)\n",
    "        mask = ~torch.all(seq_true==0, axis=2)\n",
    "        seq_pred = model(seq_true)\n",
    "\n",
    "        if loss_reduction == 'stay_wise_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "            lens = mask.sum(axis=1).detach().cpu()\n",
    "            c_lens = lens.cumsum(dim=0)\n",
    "            loss = 0\n",
    "            for idx, i in enumerate(c_lens):\n",
    "                s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                loss += l[s:c_lens[idx]].sum()/lens[idx]\n",
    "        elif loss_reduction == 'global_mean':\n",
    "            l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "            loss = l/mask.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    val_losses = []\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq_true, _, _, _ in data_loaders['val_tr']:\n",
    "            seq_true = seq_true.to(device)\n",
    "            mask = ~torch.all(seq_true==0, axis=2)\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            if loss_reduction == 'stay_wise_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum(axis=1)\n",
    "                lens = mask.sum(axis=1).detach().cpu()\n",
    "                c_lens = lens.cumsum(dim=0)\n",
    "                loss = 0\n",
    "                for idx, i in enumerate(c_lens):\n",
    "                    s = 0 if idx == 0 else c_lens[idx-1]\n",
    "                    loss += l[s:c_lens[idx]].sum()/lens[idx]\n",
    "            elif loss_reduction == 'global_mean':\n",
    "                l = criterion(seq_pred[mask], seq_true[mask]).sum()\n",
    "                loss = l/mask.sum()\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Current learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss.')\n",
    "            break\n",
    "    \n",
    "    print(f'early_stopping_counter: {early_stopping_counter} ')\n",
    "    \n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth')\n",
    "\n",
    "with open(f'model_history-lstm_ae-low_esi{ver}-{low_esi}.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4abc68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 409.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 256\n",
    "\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'val_th' #tst val_th\n",
    "\n",
    "eval_data = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, y, id, n_seq in tqdm(data_loaders[eval_split]):\n",
    "        id = id.cpu().numpy().ravel()[0]\n",
    "        y = y.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data.append([id, y, loss.item(), n_seq])\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data, columns=['id', 'true', 'score', 'n_seq'])\n",
    "eval_data.to_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce13ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 429.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluation data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed_everything(SEED)\n",
    "hidden_unit = 256\n",
    "\n",
    "model = RecurrentAutoencoder(max_seq_len, n_var, hidden_unit)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'model_best-lstm_ae-low_esi{ver}-{low_esi}.pth'))\n",
    "model = model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# loss calculation\n",
    "eval_split = 'tst' #tst val_th\n",
    "\n",
    "eval_data = []\n",
    "with torch.no_grad():\n",
    "    for seq_true, y, id, n_seq in tqdm(data_loaders[eval_split]):\n",
    "        id = id.cpu().numpy().ravel()[0]\n",
    "        y = y.cpu().numpy().ravel()[0]\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "        loss=criterion(seq_pred, seq_true)\n",
    "        \n",
    "        eval_data.append([id, y, loss.item(), n_seq])\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data, columns=['id', 'true', 'score', 'n_seq'])\n",
    "eval_data.to_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d1d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 75/40034 [00:07<1:06:57,  9.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(scores):\n\u001b[0;32m     14\u001b[0m     eval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(eval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39ms, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43meval_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     16\u001b[0m     tp, fp, fn, tn \u001b[38;5;241m=\u001b[39m conf_mat(tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m], tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m     eval_result\u001b[38;5;241m.\u001b[39mappend([s, tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mfn), tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mfp), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtp\u001b[38;5;241m/\u001b[39m(fp\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtp\u001b[38;5;241m+\u001b[39mfn)])\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1445\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m   1444\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1445\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:175\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:406\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:1390\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1388\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1389\u001b[0m ):\n\u001b[1;32m-> 1390\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:479\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    482\u001b[0m     ]\n\u001b[0;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:480\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 480\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    482\u001b[0m     ]\n\u001b[0;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:292\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:325\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m--> 325\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_split = 'val_th'\n",
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "eval_data = pd.read_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\")\n",
    "scores = eval_data['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data['pred'] = np.where(eval_data['score']>=s, 1, 0)\n",
    "    tmp = eval_data.groupby('id').agg({'true': lambda x: x.values[0], 'pred': 'max'}).reset_index()\n",
    "    tp, fp, fn, tn = conf_mat(tmp['true'], tmp['pred'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n",
    "eval_result.to_csv(f'eval_result-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f01aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true</th>\n",
       "      <th>score</th>\n",
       "      <th>n_seq</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000368</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316510</td>\n",
       "      <td>tensor([[[0],\\n         [0],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000368</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332049</td>\n",
       "      <td>tensor([[[ 0],\\n         [ 1],\\n         [ 2],...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398140</td>\n",
       "      <td>tensor([[[0],\\n         [0],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30003172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.583818</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30003172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.498034</td>\n",
       "      <td>tensor([[[ 0],\\n         [ 1],\\n         [ 2],...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40087</th>\n",
       "      <td>39999414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491358</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40088</th>\n",
       "      <td>39999414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440783</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40089</th>\n",
       "      <td>39999414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.389693</td>\n",
       "      <td>tensor([[[ 0],\\n         [ 1],\\n         [ 2],...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40090</th>\n",
       "      <td>39999833</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166329</td>\n",
       "      <td>tensor([[[0],\\n         [0],\\n         [0],\\n ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40091</th>\n",
       "      <td>39999833</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190056</td>\n",
       "      <td>tensor([[[ 0],\\n         [ 1],\\n         [ 2],...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40092 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  true     score  \\\n",
       "0      30000368     1  0.316510   \n",
       "1      30000368     1  0.332049   \n",
       "2      30003172     1  0.398140   \n",
       "3      30003172     1  0.583818   \n",
       "4      30003172     1  0.498034   \n",
       "...         ...   ...       ...   \n",
       "40087  39999414     1  0.491358   \n",
       "40088  39999414     1  0.440783   \n",
       "40089  39999414     1  0.389693   \n",
       "40090  39999833     0  0.166329   \n",
       "40091  39999833     0  0.190056   \n",
       "\n",
       "                                                   n_seq  pred  \n",
       "0      tensor([[[0],\\n         [0],\\n         [0],\\n ...     1  \n",
       "1      tensor([[[ 0],\\n         [ 1],\\n         [ 2],...     1  \n",
       "2      tensor([[[0],\\n         [0],\\n         [0],\\n ...     1  \n",
       "3      tensor([[[0],\\n         [1],\\n         [0],\\n ...     1  \n",
       "4      tensor([[[ 0],\\n         [ 1],\\n         [ 2],...     1  \n",
       "...                                                  ...   ...  \n",
       "40087  tensor([[[0],\\n         [1],\\n         [2],\\n ...     1  \n",
       "40088  tensor([[[0],\\n         [1],\\n         [2],\\n ...     1  \n",
       "40089  tensor([[[ 0],\\n         [ 1],\\n         [ 2],...     1  \n",
       "40090  tensor([[[0],\\n         [0],\\n         [0],\\n ...     0  \n",
       "40091  tensor([[[ 0],\\n         [ 1],\\n         [ 2],...     0  \n",
       "\n",
       "[40092 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feafbf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/39290 [00:03<1:05:12, 10.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(scores):\n\u001b[0;32m     15\u001b[0m     eval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(eval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39ms, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43meval_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     17\u001b[0m     tp, fp, fn, tn \u001b[38;5;241m=\u001b[39m conf_mat(tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m], tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m     eval_result\u001b[38;5;241m.\u001b[39mappend([s, tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mfn), tp\u001b[38;5;241m/\u001b[39m(tp\u001b[38;5;241m+\u001b[39mfp), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtp\u001b[38;5;241m/\u001b[39m(fp\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtp\u001b[38;5;241m+\u001b[39mfn)])\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1445\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m   1444\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1445\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:175\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:406\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:1390\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1388\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1389\u001b[0m ):\n\u001b[1;32m-> 1390\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:479\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    482\u001b[0m     ]\n\u001b[0;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\apply.py:480\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 480\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    482\u001b[0m     ]\n\u001b[0;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:292\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:325\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    324\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m--> 325\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:849\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 849\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mconstruct_array_type()\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:876\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m    872\u001b[0m initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    874\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m    877\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[0;32m    878\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1155\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1152\u001b[0m starts, ends \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mgenerate_slices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups)\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(starts, ends):\n\u001b[1;32m-> 1155\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chop\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1169\u001b[0m, in \u001b[0;36mSeriesSplitter._chop\u001b[1;34m(self, sdata, slice_obj)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chop\u001b[39m(\u001b[38;5;28mself\u001b[39m, sdata: Series, slice_obj: \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;66;03m# fastpath equivalent to `sdata.iloc[slice_obj]`\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m sdata\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_slice(slice_obj)\n\u001b[1;32m-> 1169\u001b[0m     ser \u001b[38;5;241m=\u001b[39m \u001b[43msdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m     ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m sdata\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39m__finalize__(sdata, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\series.py:582\u001b[0m, in \u001b[0;36mSeries._constructor_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m Series:\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;66;03m# we are pandas.Series (or a subclass that doesn't override _constructor)\u001b[39;00m\n\u001b[0;32m    584\u001b[0m         ser \u001b[38;5;241m=\u001b[39m Series\u001b[38;5;241m.\u001b[39m_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39maxes)\n\u001b[0;32m    585\u001b[0m         ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DAHS\\AppData\\Local\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\series.py:577\u001b[0m, in \u001b[0;36mSeries._constructor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_mgr, s\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Series]:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Series\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ver=2\n",
    "eval_split = 'tst'\n",
    "def conf_mat(true, pred):\n",
    "    tp = ((pred == 1) & (true == 1)).sum()\n",
    "    fp = ((pred == 1) & (true == 0)).sum()\n",
    "    fn = ((pred == 0) & (true == 1)).sum()\n",
    "    tn = ((pred == 0) & (true == 0)).sum()\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "eval_result = []\n",
    "eval_data = pd.read_csv(f\"eval_data-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv\")\n",
    "scores = eval_data['score'].unique()\n",
    "\n",
    "for s in tqdm(scores):\n",
    "    eval_data['pred'] = np.where(eval_data['score']>=s, 1, 0)\n",
    "    tmp = eval_data.groupby('id').agg({'true': lambda x: x.values[0], 'pred': 'max'}).reset_index()\n",
    "    tp, fp, fn, tn = conf_mat(tmp['true'], tmp['pred'])\n",
    "\n",
    "    eval_result.append([s, tp/(tp+fn), tp/(tp+fp), 2*tp/(fp+2*tp+fn)])\n",
    "\n",
    "eval_result = pd.DataFrame(eval_result, columns=['score', 'rec', 'prec', 'f1'])\n",
    "eval_result.to_csv(f'eval_result-low_esi{ver}-{low_esi}-lstm_ae-{eval_split}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2773b275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true</th>\n",
       "      <th>score</th>\n",
       "      <th>n_seq</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354099</td>\n",
       "      <td>tensor([[[0],\\n         [0],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30001684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385090</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30001684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368411</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30001684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313171</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30001684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.331349</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39339</th>\n",
       "      <td>39997251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277656</td>\n",
       "      <td>tensor([[[0],\\n         [0],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39340</th>\n",
       "      <td>39997251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271088</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [0],\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39341</th>\n",
       "      <td>39997251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39342</th>\n",
       "      <td>39997251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227602</td>\n",
       "      <td>tensor([[[0],\\n         [1],\\n         [2],\\n ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39343</th>\n",
       "      <td>39997251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221141</td>\n",
       "      <td>tensor([[[ 0],\\n         [ 1],\\n         [ 2],...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39344 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  true     score  \\\n",
       "0      30001684     0  0.354099   \n",
       "1      30001684     0  0.385090   \n",
       "2      30001684     0  0.368411   \n",
       "3      30001684     0  0.313171   \n",
       "4      30001684     0  0.331349   \n",
       "...         ...   ...       ...   \n",
       "39339  39997251     0  0.277656   \n",
       "39340  39997251     0  0.271088   \n",
       "39341  39997251     0  0.252475   \n",
       "39342  39997251     0  0.227602   \n",
       "39343  39997251     0  0.221141   \n",
       "\n",
       "                                                   n_seq  pred  \n",
       "0      tensor([[[0],\\n         [0],\\n         [0],\\n ...     1  \n",
       "1      tensor([[[0],\\n         [1],\\n         [0],\\n ...     1  \n",
       "2      tensor([[[0],\\n         [1],\\n         [2],\\n ...     1  \n",
       "3      tensor([[[0],\\n         [1],\\n         [2],\\n ...     1  \n",
       "4      tensor([[[0],\\n         [1],\\n         [2],\\n ...     1  \n",
       "...                                                  ...   ...  \n",
       "39339  tensor([[[0],\\n         [0],\\n         [0],\\n ...     1  \n",
       "39340  tensor([[[0],\\n         [1],\\n         [0],\\n ...     1  \n",
       "39341  tensor([[[0],\\n         [1],\\n         [2],\\n ...     0  \n",
       "39342  tensor([[[0],\\n         [1],\\n         [2],\\n ...     0  \n",
       "39343  tensor([[[ 0],\\n         [ 1],\\n         [ 2],...     0  \n",
       "\n",
       "[39344 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7a807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
